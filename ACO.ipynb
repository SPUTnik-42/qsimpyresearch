{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae24cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_creator import qsimpy_env_creator\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efe04f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class AcoSolutions:\n",
    "    def __init__(self, env, num_ants=10, num_iterations=100, evaporation_rate=0.5, pheromone_init=1.0):\n",
    "        \"\"\"\n",
    "        Initializes the ACO algorithm parameters.\n",
    "        Args:\n",
    "            - env: The environment on which the ACO will operate.\n",
    "            - num_ants (int): Number of ants used in the algorithm.\n",
    "            - num_iterations (int): Total iterations to run the algorithm.\n",
    "            - evaporation_rate (float): Rate at which pheromone decays.\n",
    "            - pheromone_init (float): Initial pheromone level.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.num_ants = num_ants\n",
    "        self.num_iterations = num_iterations\n",
    "        self.evaporation_rate = evaporation_rate\n",
    "        self.pheromone_init = pheromone_init\n",
    "        \n",
    "        # Initialize pheromone levels and solution storage\n",
    "        self.pheromones = {}  # Dictionary to store pheromone levels for each node\n",
    "        self.best_solution = None\n",
    "        self.best_cost = float(\"inf\")\n",
    "\n",
    "    def initialize_pheromones(self):\n",
    "        \"\"\"\n",
    "        Initialize pheromone levels for each node with the initial pheromone level.\n",
    "        \"\"\"\n",
    "        for node in self.env.nodes:\n",
    "            self.pheromones[node] = self.pheromone_init\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Runs the ACO algorithm for a set number of iterations.\n",
    "        \"\"\"\n",
    "        self.initialize_pheromones()\n",
    "        self.results = []\n",
    "        # Reset the subset of QTasks \n",
    "        self.env.round = 1\n",
    "\n",
    "        for iteration in range(self.num_iterations):\n",
    "            # Initialize the temporary array to store the results of the QTasks execution for each episode\n",
    "            arr_temp = {\n",
    "                \"total_completion_time\": 0.0,\n",
    "                \"rescheduling_count\": 0.0\n",
    "            }\n",
    "            terminated = False\n",
    "\n",
    "            # Reset the environment and setup the quantum resources\n",
    "            self.env.reset()\n",
    "            self.env.setup_quantum_resources()\n",
    "            self.rr_index = 0\n",
    "\n",
    "            while not terminated:\n",
    "                # Get the action with the given control\n",
    "                action = self.aco()\n",
    "        \n",
    "                self.env.reset()\n",
    "                self.env.setup_quantum_resources()\n",
    "                action = self.random()\n",
    "                obs, reward, terminated, done, info = self.env.step(action)\n",
    "                \n",
    "                \n",
    "                self.env.qsp_env.run()\n",
    "    \n",
    "    def aco(self):\n",
    "        \"\"\"\n",
    "        Runs the ACO algorithm for a set number of iterations.\n",
    "        \"\"\"\n",
    "        self.initialize_pheromones()\n",
    "        \n",
    "        for iteration in range(self.num_iterations):\n",
    "            solutions = []\n",
    "            for _ in range(self.num_ants):\n",
    "                solution, cost = self.construct_solution()\n",
    "                solutions.append((solution, cost))\n",
    "                if cost < self.best_cost:\n",
    "                    self.best_solution = solution\n",
    "                    self.best_cost = cost\n",
    "            self.update_pheromones(solutions)\n",
    "\n",
    "    def construct_solution(self):\n",
    "        \"\"\"\n",
    "        Constructs a solution for a single ant based on pheromone levels and completion times.\n",
    "        Returns:\n",
    "            - solution (list): The sequence of nodes visited by the ant.\n",
    "            - cost (float): The total completion time associated with this path.\n",
    "        \"\"\"\n",
    "        path = []\n",
    "        total_time = 0\n",
    "        \n",
    "        # Start from a random node\n",
    "        unvisited_nodes = set(self.env.qnodes)\n",
    "        current_node = random.choice(list(unvisited_nodes))\n",
    "        unvisited_nodes.remove(current_node)\n",
    "        path.append(current_node)\n",
    "        \n",
    "        while unvisited_nodes:\n",
    "            next_node = self.choose_next_node(unvisited_nodes)\n",
    "            path.append(next_node)\n",
    "            total_time += self.env.get_completion_time(next_node)\n",
    "            current_node = next_node\n",
    "            unvisited_nodes.remove(current_node)\n",
    "\n",
    "        return path, total_time\n",
    "\n",
    "    def choose_next_node(self, unvisited_nodes):\n",
    "        \"\"\"\n",
    "        Chooses the next node for an ant to visit based on pheromone levels and completion times.\n",
    "        Args:\n",
    "            - current_node: The current node where the ant is located.\n",
    "            - unvisited_nodes: Set of nodes not yet visited by the ant.\n",
    "        Returns:\n",
    "            - next_node: The chosen next node.\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        \n",
    "        for node in unvisited_nodes:\n",
    "            alpha = \n",
    "            beta = \n",
    "            pheromone = self.pheromones[node] ** self.alpha\n",
    "            heuristic = (1 / self.env.get_completion_time(node)) ** self.beta  # Inverse of completion time as heuristic\n",
    "            probabilities.append((node, pheromone * heuristic))\n",
    "        \n",
    "        # Normalize probabilities\n",
    "        total_probability = sum(prob[1] for prob in probabilities)\n",
    "        probabilities = [(node, prob / total_probability) for node, prob in probabilities]\n",
    "\n",
    "        # Roulette-wheel selection for next node\n",
    "        choice = random.uniform(0, 1)\n",
    "        cumulative_probability = 0\n",
    "        for node, probability in probabilities:\n",
    "            cumulative_probability += probability\n",
    "            if choice <= cumulative_probability:\n",
    "                return node\n",
    "\n",
    "    def update_pheromones(self, solutions):\n",
    "        \"\"\"\n",
    "        Updates pheromone levels on nodes visited by ants, applying evaporation and reinforcement.\n",
    "        Args:\n",
    "            - solutions (list): List of tuples with (path, cost) for each ant's solution.\n",
    "        \"\"\"\n",
    "        # Evaporate pheromones\n",
    "        for node in self.pheromones:\n",
    "            self.pheromones[node] *= (1 - self.evaporation_rate)\n",
    "\n",
    "        # Add pheromones based on solutions\n",
    "        for path, cost in solutions:\n",
    "            for node in path:\n",
    "                self.pheromones[node] += 1.0 / cost\n",
    "\n",
    "    def get_best_solution(self):\n",
    "        \"\"\"\n",
    "        Returns the best solution found by the algorithm.\n",
    "        \"\"\"\n",
    "        return self.best_solution, self.best_cost\n",
    "\n",
    "\n",
    "    def _plot_results(self, paths) -> None:\n",
    "        \"\"\"\n",
    "        Plot the results of the episodes.\n",
    "        \"\"\"\n",
    "        for path in paths:\n",
    "            df1 = pd.read_csv(path['path'])\n",
    "\n",
    "            plt.plot(df1['Episode'], df1['Total Completion Time'], \".-\", color=path['color'], label=path['label'])\n",
    "\n",
    "            self._summarize_results(df1, path['label'])\n",
    "        \n",
    "        plt.ylabel('Total Completion Time')\n",
    "        plt.xlabel('Evaluation Episode')\n",
    "        plt.legend(loc=2)\n",
    "        plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(10))\n",
    "        plt.show()\n",
    "\n",
    "    def _summarize_results(self, values, label) -> None:\n",
    "        \"\"\"\n",
    "        Summarize the results of the episodes.\n",
    "        \"\"\"\n",
    "        print(\"Results Summary for\" + label + \"solution:\")\n",
    "        print(f\"Number of Episodes: {self.num_episodes}\")\n",
    "        print(f\"Total Completion Time: {sum(values['Total Completion Time'])}\")\n",
    "        print(f\"Average Rescheduling Count: {sum(values['Rescheduling Count']) / self.num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f71e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeuristicSolutions:\n",
    "    def __init__(self, env, num_episodes=100):\n",
    "\n",
    "        # Initialize the environment\n",
    "        self.env = env\n",
    "        self.num_episodes = num_episodes\n",
    "\n",
    "        # Initialize the results of heuristic solutions\n",
    "        self.results = []\n",
    "        # Round Robin index for the QNodes. Example: [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, ...]\n",
    "        self.rr_index = 0\n",
    "        # Priority index of Greedy solution after sorting the QNodes based on the waiting time\n",
    "        self.greedy_index = 0\n",
    "\n",
    "    def run(self, control):\n",
    "        \"\"\"\n",
    "        Run the heuristic solutions for the given algorithm (control).\n",
    "        Args:\n",
    "            - control (str): The heuristic algorithm to use. Options: \"greedy\", \"random\", \"round_robin\", \"greedy_error\"\n",
    "        \"\"\"\n",
    "\n",
    "        self.results = []\n",
    "        # Reset the subset of QTasks \n",
    "        self.env.round = 1\n",
    "\n",
    "        for _ in range(self.num_episodes):\n",
    "\n",
    "            # Initialize the temporary array to store the results of the QTasks execution for each episode\n",
    "            arr_temp = {\n",
    "                \"total_completion_time\": 0.0,\n",
    "                \"rescheduling_count\": 0.0\n",
    "            }\n",
    "            terminated = False\n",
    "\n",
    "            # Reset the environment and setup the quantum resources\n",
    "            self.env.reset()\n",
    "            self.env.setup_quantum_resources()\n",
    "            self.rr_index = 0\n",
    "\n",
    "            while not terminated:\n",
    "                # Get the action with the given control\n",
    "                if control == \"greedy\":\n",
    "                    action = self.greedy(self.greedy_index)\n",
    "                elif control == \"random\":\n",
    "                    action = self.random()\n",
    "                elif control == \"round_robin\":\n",
    "                    action = self.round_robin()\n",
    "                elif control == \"greedy_error\":\n",
    "                    action = self.greedy_error(self.greedy_index)\n",
    "                \n",
    "                obs, reward, terminated, done, info = self.env.step(action)\n",
    "                \n",
    "                # If the QNode is busy or not satisfied, move to the next priority QNode\n",
    "                self.greedy_index += 1\n",
    "                if reward > 0:\n",
    "                    \"\"\"Get the results of the QTask execution\n",
    "\n",
    "                    Values:\n",
    "                        - Total Completion Time: waiting_time + execution_time\n",
    "                        - Rescheduling Count: rescheduling_count\n",
    "                    \"\"\"\n",
    "                    # Reset priority index of Greedy solution if QTasks are satisfied\n",
    "                    self.greedy_index = 0\n",
    "\n",
    "                    arr_temp[\"total_completion_time\"] += info[\"scheduled_qtask\"].waiting_time + info[\"scheduled_qtask\"].execution_time\n",
    "                    arr_temp[\"rescheduling_count\"] += info[\"scheduled_qtask\"].rescheduling_count\n",
    "            self.env.qsp_env.run()\n",
    "            # Final results of the episode\n",
    "            self.results.append(arr_temp)\n",
    "\n",
    "        # Save the results to a CSV file\n",
    "        self._save_to_csv(control)\n",
    "                \n",
    "    def greedy(self, greedy_index):\n",
    "        # Sort the QNodes based on the next available time (or waiting time) and select the QNode with the smallest waiting time\n",
    "        greedy_strategy = sorted(self.env.qnodes, key=lambda x: x.next_available_time)\n",
    "        return self.env.qnodes.index(greedy_strategy[greedy_index])\n",
    "\n",
    "    def random(self):\n",
    "        # Randomly select a QNode\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "    \n",
    "    def round_robin(self):\n",
    "        # Select the QNode based on the Round Robin index\n",
    "        action = self.rr_index % self.env.n_qnodes\n",
    "        self.rr_index += 1\n",
    "        return action\n",
    "    \n",
    "    def greedy_error(self, greedy_index, g_error=\"Readout_assignment_error\"):\n",
    "        # Sort the QNodes based on the next available time (or waiting time) and select the QNode with the \n",
    "        # smallest waiting time and smallest error (default is readout_error) in the qnode\n",
    "    \n",
    "        greedy_strategy = sorted(self.env.qnodes, key=lambda x: (x.next_available_time, x.error[g_error]))\n",
    "        return self.env.qnodes.index(greedy_strategy[greedy_index])\n",
    "\n",
    "    def _save_to_csv(self, control) -> None:\n",
    "        \"\"\"\n",
    "        Save values and episodes to a CSV file.\n",
    "        \"\"\"\n",
    "\n",
    "        file_name = \"./results/heuristics2/\" \n",
    "\n",
    "        if not os.path.exists(file_name):\n",
    "            os.makedirs(file_name)\n",
    "\n",
    "        file_name += control + \".csv\"\n",
    "        # Open the CSV file in write mode\n",
    "        with open(file_name, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            \n",
    "            # Write the header\n",
    "            writer.writerow(['Episode', 'Total Completion Time', 'Rescheduling Count'])\n",
    "            \n",
    "            # Write the data\n",
    "            for i in range(len(self.results)):\n",
    "                writer.writerow([i, self.results[i]['total_completion_time'], self.results[i]['rescheduling_count']])\n",
    "        print(\"CSV file saved to \" + file_name)\n",
    "\n",
    "    def _plot_results(self, paths) -> None:\n",
    "        \"\"\"\n",
    "        Plot the results of the episodes.\n",
    "        \"\"\"\n",
    "        for path in paths:\n",
    "            df1 = pd.read_csv(path['path'])\n",
    "\n",
    "            plt.plot(df1['Episode'], df1['Total Completion Time'], \".-\", color=path['color'], label=path['label'])\n",
    "\n",
    "            self._summarize_results(df1, path['label'])\n",
    "        \n",
    "        plt.ylabel('Total Completion Time')\n",
    "        plt.xlabel('Evaluation Episode')\n",
    "        plt.legend(loc=2)\n",
    "        plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(10))\n",
    "        plt.show()\n",
    "\n",
    "    def _summarize_results(self, values, label) -> None:\n",
    "        \"\"\"\n",
    "        Summarize the results of the episodes.\n",
    "        \"\"\"\n",
    "        print(\"Results Summary for\" + label + \"solution:\")\n",
    "        print(f\"Number of Episodes: {self.num_episodes}\")\n",
    "        print(f\"Total Completion Time: {sum(values['Total Completion Time'])}\")\n",
    "        print(f\"Average Rescheduling Count: {sum(values['Rescheduling Count']) / self.num_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7a133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\Music\\Qintern\\research\\qsimpy\\qenv\\lib\\site-packages\\gymnasium\\spaces\\box.py:130: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  gym.logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Run the heuristic solutions\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# heuristics = HeuristicSolutions(env, num_episodes=100)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# heuristics.run(\"greedy\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#Run the ACO \u001b[39;00m\n\u001b[0;32m     20\u001b[0m aco \u001b[38;5;241m=\u001b[39m ACOSolution(env, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, num_ants\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43maco\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Plot the results\u001b[39;00m\n\u001b[0;32m     24\u001b[0m paths \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# {\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m#     \"label\": \"random\",\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[2], line 42\u001b[0m, in \u001b[0;36mACOSolution.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m best_node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_aco_iteration(current_task, pheromone_matrix)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Take action in environment\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m obs, reward, terminated, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# Update results if task was successfully scheduled\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     episode_result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_completion_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     47\u001b[0m         info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduled_qtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mwaiting_time \u001b[38;5;241m+\u001b[39m \n\u001b[0;32m     48\u001b[0m         info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduled_qtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mexecution_time\n\u001b[0;32m     49\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hp\\Music\\Qintern\\research\\qsimpy\\qenv\\lib\\site-packages\\gymnasium\\core.py:469\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    467\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\hp\\Music\\Qintern\\research\\qsimpy\\qenv\\lib\\site-packages\\gymnasium\\core.py:469\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    467\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    468\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the :attr:`env` after calling :meth:`step` using :meth:`self.observation` on the returned observations.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 469\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n",
      "File \u001b[1;32mc:\\Users\\hp\\Music\\Qintern\\research\\qsimpy\\gymenv_qsimpy.py:321\u001b[0m, in \u001b[0;36mQSimPyEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_qtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs, reward, terminated, \u001b[38;5;28;01mFalse\u001b[39;00m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduled_qtask\u001b[39m\u001b[38;5;124m\"\u001b[39m: scheduled_qtask}\n",
      "File \u001b[1;32mc:\\Users\\hp\\Music\\Qintern\\research\\qsimpy\\gymenv_qsimpy.py:159\u001b[0m, in \u001b[0;36mQSimPyEnv._get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnode_obs\u001b[38;5;241m.\u001b[39mappend(qnode_obs)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;66;03m# Flatten the qnode observations and concatenate with qtask observations\u001b[39;00m\n\u001b[1;32m--> 159\u001b[0m qnode_obs_flat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnode_obs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(\n\u001b[0;32m    161\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqtask_obs, qnode_obs_flat), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64\n\u001b[0;32m    162\u001b[0m )\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_obs\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create the QSimPy environment\n",
    "    env_config={\n",
    "                \"obs_filter\": \"rescale_-1_1\",\n",
    "                \"reward_filter\": None,\n",
    "                \"dataset\": \"qdataset/qsimpyds_100_sub_24.csv\",\n",
    "            }\n",
    "\n",
    "    env = qsimpy_env_creator(env_config)\n",
    "\n",
    "    # Run the heuristic solutions\n",
    "    # heuristics = HeuristicSolutions(env, num_episodes=100)\n",
    "    # heuristics.run(\"greedy\")\n",
    "    # heuristics.run(\"random\")\n",
    "    # heuristics.run(\"round_robin\")\n",
    "    # heuristics.run(\"greedy_error\")\n",
    "\n",
    "    #Run the ACO \n",
    "    aco = ACOSolution(env, num_episodes=10, num_ants=3, num_iterations=5)\n",
    "    aco.run()\n",
    "\n",
    "    # Plot the results\n",
    "    paths = [\n",
    "        # {\n",
    "        #     \"label\": \"random\",\n",
    "        #     \"path\": \"./results/heuristics2/random.csv\",\n",
    "        #     \"color\": \"red\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"label\": \"round robin\",\n",
    "        #     \"path\": \"./results/heuristics2/round_robin.csv\",\n",
    "        #     \"color\": \"blue\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"label\": \"greedy\",\n",
    "        #     \"path\": \"./results/heuristics2/greedy.csv\",\n",
    "        #     \"color\": \"black\"\n",
    "        # },\n",
    "        # {\n",
    "        #     \"label\": \"greedy_error\",\n",
    "        #     \"path\": \"./results/heuristics2/greedy_error.csv\",\n",
    "        #     \"color\": \"green\"\n",
    "        # },\n",
    "        {\n",
    "           \"label\": \"ACO\", \n",
    "           \"path\": \"./results/heuristics2/aco.csv\", \n",
    "           \"color\": \"purple\"\n",
    "        }\n",
    "\n",
    "    ]\n",
    "    #heuristics._plot_results(paths)\n",
    "    aco._plot_results(paths)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
